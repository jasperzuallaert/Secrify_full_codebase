{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing v1.1 and v1.2\n",
    "Changes:\n",
    "- v1.1 and v1.2: no filtering of 2023-set negatives based on positives of the old set\n",
    "- v1.2: no minimum length added\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from subprocess import DEVNULL, STDOUT, check_call\n",
    "pos_dataset = '../original_data/enriched.txt'\n",
    "neg_dataset = '../original_data/depleted.txt'\n",
    "old_pos_dataset = '../original_data/Pp_resultstable_enriched.txt'\n",
    "MIN_LEN=50\n",
    "out_dir = '../processed_data/'\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_fasta_for_clustering(df):\n",
    "    with open('tmp_cdhit.fasta','w') as write_to:\n",
    "        for idx, row in df.iterrows():\n",
    "            print(f'>{int(row.INDEX)}\\n{row.SEQ}',file=write_to)\n",
    "\n",
    "def run_cdhit(cdhit_cutoff):\n",
    "    # check_call(['cd-hit', '-i tmp_cdhit.fasta', '-o tmp_cdhit.out', f'-c {cdhit_cutoff}', f'-s {cdhit_cutoff}', f'-d 0'], stdout=DEVNULL, stderr=STDOUT, shell=True)\n",
    "    !echo \"Running cdhit with\" {cdhit_cutoff}\n",
    "    !cd-hit -i tmp_cdhit.fasta -o tmp_cdhit.out -c {cdhit_cutoff} -s {cdhit_cutoff} -d 0 &>/dev/null\n",
    "    \n",
    "def process_cdhit_outputs():\n",
    "    results = []\n",
    "    cluster_number = None\n",
    "    for line in open('tmp_cdhit.out.clstr'):\n",
    "        if not line.startswith('>'):\n",
    "            results.append((int(line[line.index('>')+1:line.index('...')]), cluster_number))\n",
    "        else:\n",
    "            cluster_number = int(line.split(' ')[1].strip())\n",
    "    ret = pd.DataFrame(results, columns=['INDEX', 'CLUSTER'])\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_records(main_pos_file, main_neg_file, fc_cutoff, cdhit_cutoff, old_pos_file=None, old_neg_file=None):\n",
    "    print(f'FC cutoff: {fc_cutoff}, arbitrary clustering cutoff: {cdhit_cutoff}')\n",
    "    # new data\n",
    "    df_pos = pd.read_csv(main_pos_file, sep='\\t')\n",
    "    df_pos.drop_duplicates(inplace=True)\n",
    "    df_neg = pd.read_csv(main_neg_file, sep='\\t')\n",
    "    df_neg.drop_duplicates(inplace=True)\n",
    "\n",
    "    df_pos = df_pos[df_pos['protein'].str.len()>=MIN_LEN]\n",
    "    df_neg = df_neg[df_neg['protein'].str.len()>=MIN_LEN]\n",
    "    df_pos['label'] = 1\n",
    "    df_neg['label'] = 0\n",
    "    if 'logFC_P1' in df_pos.columns:\n",
    "        df_pos = df_pos[(df_pos.logFC_P1>=fc_cutoff) & (df_pos.logFC_P3>=fc_cutoff)]\n",
    "        df_neg = df_neg[(df_neg.logFC_N1>=fc_cutoff) & (df_neg.logFC_N3>=fc_cutoff)]\n",
    "    else:\n",
    "        df_pos = df_pos[(df_pos.logFC_rep3>=fc_cutoff) & (df_pos.logFC_rep4>=fc_cutoff) & (df_pos.logFC_rep5>=fc_cutoff)]\n",
    "        df_neg = df_neg[(df_neg.logFC_rep3<=-fc_cutoff) & (df_neg.logFC_rep4<=-fc_cutoff) & (df_neg.logFC_rep5<=-fc_cutoff)]\n",
    "\n",
    "    if old_pos_file and old_neg_file:\n",
    "        # old data\n",
    "        old_df_pos = pd.read_csv(old_pos_dataset, sep='\\t')\n",
    "        old_df_pos.drop_duplicates(inplace=True)\n",
    "        old_df_pos = old_df_pos[old_df_pos['protein'].str.len()>=MIN_LEN]\n",
    "        old_df_pos['label'] = 1\n",
    "        old_df_pos = old_df_pos[(old_df_pos.logFC_rep3>=fc_cutoff) & (old_df_pos.logFC_rep4>=fc_cutoff) & (old_df_pos.logFC_rep5>=fc_cutoff)]\n",
    "\n",
    "        # merge\n",
    "        old_df_pos = old_df_pos[['protein', 'label']]\n",
    "        old_df_pos.columns = ['SEQ', 'LABEL']\n",
    "    \n",
    "    df_pos = df_pos[['protein', 'label']]\n",
    "    df_neg = df_neg[['protein', 'label']]\n",
    "    df = pd.concat([df_pos, df_neg]).reset_index()\n",
    "    df.columns = ['INDEX', 'SEQ', 'LABEL']\n",
    "\n",
    "    new_df = df\n",
    "    # get clusters for new dataset only\n",
    "    write_fasta_for_clustering(new_df)\n",
    "    run_cdhit(cdhit_cutoff)\n",
    "    cluster_info = process_cdhit_outputs()\n",
    "    new_df = new_df.merge(cluster_info, on='INDEX', how='left')\n",
    "\n",
    "    # get clusters with inconsistent labels\n",
    "    unique_labels_per_cluster = new_df.groupby(\"CLUSTER\").LABEL.nunique()\n",
    "    unique_labels_per_cluster.name = 'unique_labels'\n",
    "    new_df = new_df.merge(unique_labels_per_cluster, on='CLUSTER', how='left')\n",
    "\n",
    "    # do analysis on those labels (numbers)\n",
    "    print(' > numbers on new dataset')\n",
    "    new_df['source'] = 'new'\n",
    "\n",
    "    # filter out those clusters\n",
    "    filtered_df = new_df[new_df.unique_labels==1]\n",
    "    filtered_df = filtered_df[['SEQ', 'LABEL', 'source']]\n",
    "\n",
    "    if old_pos_file and old_neg_file:\n",
    "        print('TMP### adding old enriched data')\n",
    "        # construct full dataset with enriched from previous set\n",
    "        old_df_pos['source'] = 'old'\n",
    "        full_df = pd.concat([filtered_df, old_df_pos])\n",
    "        full_df.reset_index(inplace=True)\n",
    "        full_df.columns = ['INDEX', 'SEQ', 'LABEL', 'source']\n",
    "\n",
    "        # get clusters with inconsistent labels\n",
    "        write_fasta_for_clustering(full_df)\n",
    "        run_cdhit(cdhit_cutoff)\n",
    "        cluster_info = process_cdhit_outputs()\n",
    "        full_df = full_df.merge(cluster_info, on='INDEX', how='left')\n",
    "        unique_labels_per_cluster = full_df.groupby(\"CLUSTER\").LABEL.nunique()\n",
    "        unique_labels_per_cluster.name = 'unique_labels'\n",
    "        full_df = full_df.merge(unique_labels_per_cluster, on='CLUSTER', how='left')\n",
    "\n",
    "        # do analysis on those labels (numbers)\n",
    "        print(' > numbers on new dataset + old enriched')\n",
    "\n",
    "        # filter out those clusters\n",
    "        final_df = full_df[full_df.unique_labels==1]\n",
    "        final_df = final_df[final_df.source == 'new']\n",
    "    else:\n",
    "        filtered_df.reset_index(inplace=True)\n",
    "        filtered_df.columns = ['INDEX', 'SEQ', 'LABEL', 'source']\n",
    "        final_df = filtered_df\n",
    "\n",
    "    # get final numbers\n",
    "    print(' > final numbers')\n",
    "    print(' - Number of enriched entries: {}'.format(len(final_df[final_df.LABEL==1].drop_duplicates())))\n",
    "    print(' - Number of depleted entries: {}'.format(len(final_df[final_df.LABEL==0].drop_duplicates())))\n",
    "\n",
    "    return final_df, new_df\n",
    "\n",
    "results = []\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 1.0 and 5.0 datasets, where there is no overlap between train/valid/test within one, and between train/valid and test splits between the two (when clustering similar sequences at 0.7)\n",
    "div = [0.7, 0.1, 0.2]\n",
    "df_at_fc1, _ = filter_records('../data/enriched.txt', '../data/depleted.txt', 1, 0.97)\n",
    "df_at_fc5, _ = filter_records('../data/enriched.txt', '../data/depleted.txt', 5, 0.97)\n",
    "\n",
    "df_at_fc5.INDEX = df_at_fc5.INDEX + 1000000\n",
    "\n",
    "write_fasta_for_clustering(df_at_fc1)\n",
    "\n",
    "cdhit_cutoff = 0.7\n",
    "run_cdhit(cdhit_cutoff)\n",
    "cluster_info = process_cdhit_outputs()\n",
    "cluster_info.columns = ['INDEX', 'CLUSTER_FC1']\n",
    "print(cluster_info)\n",
    "df_at_fc1 = df_at_fc1.merge(cluster_info, on='INDEX', how='left')\n",
    "\n",
    "write_fasta_for_clustering(df_at_fc5)\n",
    "run_cdhit(cdhit_cutoff)\n",
    "cluster_info = process_cdhit_outputs()\n",
    "cluster_info.columns = ['INDEX', 'CLUSTER_FC5']\n",
    "df_at_fc5 = df_at_fc5.merge(cluster_info, on='INDEX', how='left')\n",
    "\n",
    "combined_fc1_fc5_start = pd.concat([df_at_fc1, df_at_fc5])\n",
    "write_fasta_for_clustering(combined_fc1_fc5_start)\n",
    "run_cdhit(cdhit_cutoff)\n",
    "cluster_info = process_cdhit_outputs()\n",
    "cluster_info.columns = ['INDEX', 'CLUSTER_FC1_FC5']\n",
    "combined_fc1_fc5_start = combined_fc1_fc5_start.merge(cluster_info, on='INDEX', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_fc1_fc5 = combined_fc1_fc5_start.copy()\n",
    "# given a dataframe with three columns CLUSTER_FC1, CLUSTER_FC5, CLUSTER_FC1_FC5. \n",
    "# group by CLUSTER_FC1_FC5, and join clusters when entries are in the same CLUSTER_FC1 or CLUSTER_FC5\n",
    "done = False\n",
    "clustermania = combined_fc1_fc5.groupby('CLUSTER_FC1_FC5',as_index=False).agg(set)[['CLUSTER_FC1_FC5', 'CLUSTER_FC1', 'CLUSTER_FC5']]\n",
    "# clustermania.CLUSTER_FC1 = clustermania.CLUSTER_FC1.map(lambda x: {x for x in x if x>=0})\n",
    "# clustermania.CLUSTER_FC5 = clustermania.CLUSTER_FC5.map(lambda x: {x for x in x if x>=0})\n",
    "clustermania['FINAL_CLUSTER'] = clustermania.CLUSTER_FC1_FC5\n",
    "\n",
    "grouped_on_fc1 = combined_fc1_fc5.groupby('CLUSTER_FC1',as_index=False).agg(set)[['CLUSTER_FC1_FC5', 'CLUSTER_FC1', 'CLUSTER_FC5']]\n",
    "fc1fc5_groups_to_combine = list(grouped_on_fc1[grouped_on_fc1.apply(lambda x: len(x.CLUSTER_FC1_FC5)>1,axis=1)].CLUSTER_FC1_FC5)\n",
    "for group in fc1fc5_groups_to_combine:\n",
    "    clustermania.loc[clustermania.FINAL_CLUSTER.isin(group), 'FINAL_CLUSTER'] = list(group)[0]\n",
    "    \n",
    "grouped_on_fc5 = combined_fc1_fc5.groupby('CLUSTER_FC5',as_index=False).agg(set)[['CLUSTER_FC1_FC5', 'CLUSTER_FC1', 'CLUSTER_FC5']]\n",
    "fc1fc5_groups_to_combine = list(grouped_on_fc5[grouped_on_fc5.apply(lambda x: len(x.CLUSTER_FC1_FC5)>1,axis=1)].CLUSTER_FC1_FC5)\n",
    "for group in fc1fc5_groups_to_combine:\n",
    "    clustermania.loc[clustermania.FINAL_CLUSTER.isin(group), 'FINAL_CLUSTER'] = list(group)[0]\n",
    "\n",
    "combined_fc1_fc5 = combined_fc1_fc5.merge(clustermania[['CLUSTER_FC1_FC5', 'FINAL_CLUSTER']], on='CLUSTER_FC1_FC5', how='left')\n",
    "# fc1_only_with_final_cluster = combined_fc1_fc5[combined_fc1_fc5.INDEX < 1000000]\n",
    "# fc5_only_with_final_cluster = combined_fc1_fc5[combined_fc1_fc5.INDEX >= 1000000]\n",
    "\n",
    "combined_fc1_fc5.loc[combined_fc1_fc5.INDEX <  1000000, 'template'] = f'{out_dir}/2023_fc1_{\"{}\"}.csv'\n",
    "combined_fc1_fc5.loc[combined_fc1_fc5.INDEX >= 1000000, 'template'] = f'{out_dir}/2023_fc5_{\"{}\"}.csv'\n",
    "combined_fc1_fc5 = combined_fc1_fc5[['INDEX', 'SEQ', 'LABEL', 'FINAL_CLUSTER', 'template']]\n",
    "combined_fc1_fc5.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_clusters(df, cluster_col, div):\n",
    "    cluster_sizes = df.groupby(cluster_col,as_index=False).agg(len)[['SEQ', 'LABEL', cluster_col]]\t\n",
    "    # divide cluster_sizes in three groups according to a split dictated by div\n",
    "    # shuffle cluster_sizes first to avoid bias\n",
    "    cluster_sizes = cluster_sizes.sample(frac=1)\n",
    "    cluster_sizes['cumsum'] = cluster_sizes.SEQ.cumsum()\n",
    "    cluster_sizes['cumsum'] = cluster_sizes['cumsum'] / cluster_sizes.SEQ.sum()\n",
    "    cluster_sizes['set'] = 'train'\n",
    "    cluster_sizes.loc[cluster_sizes['cumsum'] > div[0], 'set'] = 'valid'\n",
    "    cluster_sizes.loc[cluster_sizes['cumsum'] > div[0]+div[1], 'set'] = 'test'\n",
    "    cluster_sizes = cluster_sizes[[cluster_col, 'set']]\n",
    "\n",
    "    df = df.merge(cluster_sizes, on=cluster_col, how='left')\n",
    "\n",
    "\n",
    "    for template in df.template.unique():\n",
    "        sub_df = df[df.template==template]\n",
    "\n",
    "        ### filter within one sub_df on cdhit clusters (0.97)\n",
    "        cdhit_cutoff = 0.97\n",
    "        write_fasta_for_clustering(sub_df)\n",
    "        run_cdhit(cdhit_cutoff)\n",
    "        cluster_info = process_cdhit_outputs()\n",
    "        cluster_info.columns = ['INDEX', 'CLUSTER']\n",
    "        sub_df = sub_df.merge(cluster_info, on='INDEX', how='left')\n",
    "        # keep longest sequence per cluster\n",
    "        sub_df = sub_df.sort_values('SEQ').groupby('CLUSTER', as_index=False).last()\n",
    "        print(sub_df)\n",
    "        #######\n",
    "\n",
    "        sub_df[sub_df.set=='train'][['SEQ', 'LABEL']].to_csv(template.format('train'), index=False)\n",
    "        sub_df[sub_df.set=='valid'][['SEQ', 'LABEL']].to_csv(template.format('valid'), index=False)\n",
    "        sub_df[sub_df.set=='test'][['SEQ', 'LABEL']].to_csv(template.format('test'), index=False)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "div = [0.7, 0.1, 0.2]\n",
    "combined_fc1_fc5_with_sets = divide_clusters(combined_fc1_fc5, 'FINAL_CLUSTER', div)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_fc1_fc5_with_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqx='DKDDDTTRVDESLNIKVEAEEEKAKSGDETNKEEDEDDEEAEEEEEEEEEEEDEDDDD'\n",
    "combined_fc1_fc5[combined_fc1_fc5.SEQ.str.contains(\"DKDDDTTRVDESLNIKVEAEEEKAKSGDETNKEEDEDDEEAEEEEEEEEEEEDEDDDD\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NOW FOR OLD P.P. DATA ###\n",
    "### THING HERE IS THAT IT NEEDS TO DO THE ABOVE STUFF, BUT ALSO MAKE SURE THERE ARE NO TRAINING / VALIDATION SEQUENCES FROM THE NEW DATASETS IN THE TEST SET ###\n",
    "old_df_at_fc1, _ = filter_records('../data/Pp_resultstable_enriched.txt', '../data/Pp_resultstable_depleted.txt', 1, 0.97)\n",
    "old_df_at_fc5, _ = filter_records('../data/Pp_resultstable_enriched.txt', '../data/Pp_resultstable_depleted.txt', 5, 0.97)\n",
    "\n",
    "old_df_at_fc1.INDEX = old_df_at_fc1.INDEX + 2000000\n",
    "old_df_at_fc5.INDEX = old_df_at_fc5.INDEX + 3000000\n",
    "\n",
    "write_fasta_for_clustering(old_df_at_fc1)\n",
    "cdhit_cutoff = 0.7\n",
    "run_cdhit(cdhit_cutoff)\n",
    "cluster_info = process_cdhit_outputs()\n",
    "cluster_info.columns = ['INDEX', 'CLUSTER_FC1']\n",
    "old_df_at_fc1 = old_df_at_fc1.merge(cluster_info, on='INDEX', how='left')\n",
    "\n",
    "write_fasta_for_clustering(old_df_at_fc5)\n",
    "run_cdhit(cdhit_cutoff)\n",
    "cluster_info = process_cdhit_outputs()\n",
    "cluster_info.columns = ['INDEX', 'CLUSTER_FC5']\n",
    "old_df_at_fc5 = old_df_at_fc5.merge(cluster_info, on='INDEX', how='left')\n",
    "\n",
    "old_combined_fc1_fc5_start = pd.concat([old_df_at_fc1, old_df_at_fc5, combined_fc1_fc5_with_sets])\n",
    "write_fasta_for_clustering(old_combined_fc1_fc5_start)\n",
    "run_cdhit(cdhit_cutoff)\n",
    "cluster_info = process_cdhit_outputs()\n",
    "cluster_info.columns = ['INDEX', 'CLUSTER_FC1_FC5_and_old']\n",
    "old_combined_fc1_fc5_start = old_combined_fc1_fc5_start.merge(cluster_info, on='INDEX', how='left')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_combined_fc1_fc5_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_fc1_fc5_o = old_combined_fc1_fc5_start.copy()[['INDEX','CLUSTER_FC1_FC5_and_old', 'CLUSTER_FC1', 'CLUSTER_FC5', 'set','SEQ','LABEL']]\n",
    "# given a dataframe with three columns CLUSTER_FC1, CLUSTER_FC5, CLUSTER_FC1_FC5. \n",
    "# group by CLUSTER_FC1_FC5, and join clusters when entries are in the same CLUSTER_FC1 or CLUSTER_FC5\n",
    "done = False\n",
    "clustermania = combined_fc1_fc5_o.groupby('CLUSTER_FC1_FC5_and_old',as_index=False).agg(set)[['CLUSTER_FC1_FC5_and_old', 'CLUSTER_FC1', 'CLUSTER_FC5', 'set']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustermania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustermania['is_in_new_train_or_valid'] = clustermania.set.map(lambda x: 'train' in x or 'valid' in x)\n",
    "# clustermania.CLUSTER_FC1 = clustermania.CLUSTER_FC1.map(lambda x: {x for x in x if x>=0})\n",
    "# clustermania.CLUSTER_FC5 = clustermania.CLUSTER_FC5.map(lambda x: {x for x in x if x>=0})\n",
    "clustermania['FINAL_CLUSTER'] = clustermania.CLUSTER_FC1_FC5_and_old\n",
    "\n",
    "grouped_on_fc1 = combined_fc1_fc5_o.groupby('CLUSTER_FC1',as_index=False).agg(set)[['CLUSTER_FC1_FC5_and_old', 'CLUSTER_FC1', 'CLUSTER_FC5']]\n",
    "fc1fc5_groups_to_combine = list(grouped_on_fc1[grouped_on_fc1.apply(lambda x: len(x.CLUSTER_FC1_FC5_and_old)>1,axis=1)].CLUSTER_FC1_FC5_and_old)\n",
    "for group in fc1fc5_groups_to_combine:\n",
    "    clustermania.loc[clustermania.FINAL_CLUSTER.isin(group), 'is_in_new_train_or_valid'] = clustermania.is_in_new_train_or_valid[clustermania.FINAL_CLUSTER.isin(group)].any()\n",
    "    clustermania.loc[clustermania.FINAL_CLUSTER.isin(group), 'FINAL_CLUSTER'] = list(group)[0]\n",
    "    \n",
    "grouped_on_fc5 = combined_fc1_fc5_o.groupby('CLUSTER_FC5',as_index=False).agg(set)[['CLUSTER_FC1_FC5_and_old', 'CLUSTER_FC1', 'CLUSTER_FC5']]\n",
    "fc1fc5_groups_to_combine = list(grouped_on_fc5[grouped_on_fc5.apply(lambda x: len(x.CLUSTER_FC1_FC5_and_old)>1,axis=1)].CLUSTER_FC1_FC5_and_old)\n",
    "for group in fc1fc5_groups_to_combine:\n",
    "    clustermania.loc[clustermania.FINAL_CLUSTER.isin(group), 'is_in_new_train_or_valid'] = clustermania.is_in_new_train_or_valid[clustermania.FINAL_CLUSTER.isin(group)].any()\n",
    "    clustermania.loc[clustermania.FINAL_CLUSTER.isin(group), 'FINAL_CLUSTER'] = list(group)[0]\n",
    "\n",
    "combined_fc1_fc5_o = combined_fc1_fc5_o.merge(clustermania[['CLUSTER_FC1_FC5_and_old', 'FINAL_CLUSTER', 'is_in_new_train_or_valid']], on='CLUSTER_FC1_FC5_and_old', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_fc1_fc5_o[combined_fc1_fc5_o.SEQ==\"DKDDDTTRVDESLNIKVEAEEEKAKSGDETNKEEDEDDEEAEEEEEEEEEEEDEDDDD\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "combined_fc1_fc5_o.loc[(2000000 <= combined_fc1_fc5_o.INDEX) & (combined_fc1_fc5_o.INDEX <  3000000), 'template'] = f'{out_dir}/oldPp_fc1_{\"{}\"}.csv'\n",
    "combined_fc1_fc5_o.loc[combined_fc1_fc5_o.INDEX >= 3000000, 'template'] = f'{out_dir}/oldPp_fc5_{\"{}\"}.csv'\n",
    "combined_fc1_fc5_o = combined_fc1_fc5_o[['INDEX','SEQ', 'LABEL', 'FINAL_CLUSTER', 'template', 'is_in_new_train_or_valid']]\n",
    "combined_fc1_fc5_o.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_fc1_fc5_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_clusters_taking_into_account_existing_testsets(df, cluster_col, div):\n",
    "    cluster_sizes = df.groupby(cluster_col,as_index=False).agg(set)[['SEQ', 'LABEL', cluster_col, 'is_in_new_train_or_valid']]\t\n",
    "    cluster_sizes.SEQ = cluster_sizes.SEQ.map(len)\n",
    "    cluster_sizes.is_in_new_train_or_valid = cluster_sizes.is_in_new_train_or_valid.map(any)\n",
    "    # divide cluster_sizes in three groups according to a split dictated by div\n",
    "    # shuffle cluster_sizes first to avoid bias\n",
    "    cluster_sizes = cluster_sizes.sample(frac=1)\n",
    "    cluster_sizes.sort_values('is_in_new_train_or_valid', ascending=False, inplace=True)\n",
    "    cluster_sizes['cumsum'] = cluster_sizes.SEQ.cumsum()\n",
    "    cluster_sizes['cumsum'] = cluster_sizes['cumsum'] / cluster_sizes.SEQ.sum()\n",
    "    cluster_sizes['data_set'] = 'train'\n",
    "    cluster_sizes.loc[cluster_sizes['cumsum'] > div[0], 'data_set'] = 'valid'\n",
    "    cluster_sizes.loc[cluster_sizes['cumsum'] > div[0]+div[1], 'data_set'] = 'test'\n",
    "    cluster_sizes = cluster_sizes[[cluster_col, 'data_set']]\n",
    "\n",
    "    df = df.merge(cluster_sizes, on=cluster_col, how='left')\n",
    "    for template in df.template.unique():\n",
    "        if 'old' not in str(template):\n",
    "            continue\n",
    "        sub_df = df[df.template==template]\n",
    "        print(sub_df)\n",
    "\n",
    "        ### filter within one sub_df on cdhit clusters (0.97)\n",
    "        cdhit_cutoff = 0.97\n",
    "        write_fasta_for_clustering(sub_df)\n",
    "        run_cdhit(cdhit_cutoff)\n",
    "        cluster_info = process_cdhit_outputs()\n",
    "        cluster_info.columns = ['INDEX', 'CLUSTER']\n",
    "        sub_df = sub_df.merge(cluster_info, on='INDEX', how='left')\n",
    "        # keep longest sequence per cluster\n",
    "        sub_df = sub_df.sort_values('SEQ').groupby('CLUSTER', as_index=False).last()\n",
    "        print(sub_df)\n",
    "        #######\n",
    "\n",
    "        sub_df[sub_df.data_set=='train'][['SEQ', 'LABEL']].to_csv(template.format('train'), index=False)\n",
    "        sub_df[sub_df.data_set=='valid'][['SEQ', 'LABEL']].to_csv(template.format('valid'), index=False)\n",
    "        sub_df[sub_df.data_set=='test'][['SEQ', 'LABEL']].to_csv(template.format('test'), index=False)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputted_df_debugging = divide_clusters_taking_into_account_existing_testsets(combined_fc1_fc5_o, 'FINAL_CLUSTER', div)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_naive_enriched_df = pd.read_csv('../data/Pp_resultstable_enriched.txt', sep='\\t')\n",
    "old_naive_depleted_df = pd.read_csv('../data/Pp_resultstable_depleted.txt', sep='\\t')\n",
    "old_naive_enriched_df['LABEL'] = 1\n",
    "old_naive_depleted_df['LABEL'] = 0\n",
    "old_naive_df = pd.concat([old_naive_enriched_df, old_naive_depleted_df])\n",
    "old_naive_df = old_naive_df[['protein', 'LABEL']]\n",
    "old_naive_df.columns = ['SEQ', 'LABEL']\n",
    "old_naive_df = old_naive_df[old_naive_df.SEQ.map(len)>=MIN_LEN]\n",
    "# shuffle and write train/valid/test\n",
    "old_naive_df = old_naive_df.sample(frac=1)\n",
    "old_naive_df.iloc[:int(len(old_naive_df)*div[0])][['SEQ', 'LABEL']].to_csv(f'{out_dir}/old_naive_train.csv', index=False)\n",
    "old_naive_df.iloc[int(len(old_naive_df)*div[0]):int(len(old_naive_df)*(div[0]+div[1]))][['SEQ', 'LABEL']].to_csv(f'{out_dir}/old_naive_valid.csv', index=False)\n",
    "old_naive_df.iloc[int(len(old_naive_df)*(div[0]+div[1])):][['SEQ', 'LABEL']].to_csv(f'{out_dir}/old_naive_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_naive_df.iloc[int(len(old_naive_df)*div[0]):int(len(old_naive_df)*(div[0]+div[1]))][['SEQ', 'LABEL']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* OLD CODE FROM HERE *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
